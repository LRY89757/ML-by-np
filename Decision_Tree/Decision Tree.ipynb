{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(208, 60)\n",
      "(90, 60)\n",
      "(208, 60)\n",
      "(208,)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "class READ(object):\n",
    "    '''\n",
    "    sonar数据集\n",
    "    '''\n",
    "    def __init__(self,root,path):\n",
    "        '''\n",
    "        方法说明:\n",
    "            初始化类\n",
    "        参数说明:\n",
    "            root: 文件夹根目录\n",
    "            path: sonar数据集文件名 'sonar.csv'\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.path = path\n",
    "        self.feature, self.label = self._get_data()\n",
    "\n",
    "    def _get_data(self):\n",
    "        #打开数据集\n",
    "        with open(os.path.join(self.root,self.path),'r') as f:\n",
    "            data = f.readlines()[:]\n",
    "        feature = []\n",
    "        label = []\n",
    "        #去除掉逗号\n",
    "        for i in range(len(data)):\n",
    "            feature.append(data[i].strip().split(',')[:-1])\n",
    "            label.append(data[i].strip().split(',')[-1])\n",
    "        #转化为numpy数组格式\n",
    "        feature = np.array(feature)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        return feature, label\n",
    "\n",
    "root = \"./\"\n",
    "path = \"sonar.csv\"\n",
    "dataset = READ(root, path)\n",
    "feature = dataset.feature\n",
    "label = dataset.label\n",
    "\n",
    "\n",
    "\n",
    "# 处理数据，将数据标签数字化\n",
    "trans_label = {'M':0, 'R':1}\n",
    "label = np.array([trans_label[value] for value in label])   # 数字化label：  'M'：0， ’R‘：1\n",
    "feature = np.array([[float(i) for i in sing_feature] for sing_feature in feature])\n",
    "# print(feature.shape)\n",
    "# print(type(feature[0][0]), type(label[0]))\n",
    "# print(feature)\n",
    "\n",
    "# # 将连续型数据变为离散型\n",
    "feature_range = [(min(sing), max(sing)) for sing in feature.T]   # 每一个属性的最大值最小值的列表\n",
    "# print(feature_range)\n",
    "range_divide = [np.arange(low, high+0.5, (high - low) / 4) for low, high in feature_range] # 分为10等分，每个属性都有10类\n",
    "# print(range_divide.shape)\n",
    "# 将每一类连续型的变为离散型\n",
    "def trans_feature(sing_feature, range_divide, index):      \n",
    "    '''\n",
    "    input:\n",
    "    sing_feature:第index个属性的类别矩阵\n",
    "    range_divide:划分标准矩阵\n",
    "    output:划分好的离散型\n",
    "    '''\n",
    "    output = []\n",
    "    for sing in sing_feature:\n",
    "        for i in range(len(range_divide[index])):\n",
    "            if range_divide[index][i] > sing:\n",
    "                output.append(i - 1)\n",
    "                break\n",
    "    return np.array(output)\n",
    "# len(trans_feature(feature.T[0], range_divide, 0))\n",
    "feature = np.array([trans_feature(feature.T[index], range_divide, index) for index in range(len(feature.T))]).T\n",
    "# print(type(feature[0][0]))\n",
    "# feature = np.array([np.array([feature[i][index] for i in range[60]]) for index in range(208)])\n",
    "print(type(feature))\n",
    "print(feature.shape)\n",
    "\n",
    "# 随机抽取数据作为训练集和测试集以及验证集\n",
    "train_val = np.random.choice(range(208), 150, replace=False)        # 训练集和验证集索引\n",
    "val = np.random.choice(range(70), 50, replace=False)\n",
    "train = np.random.choice([i for i in range(150) if i not in val], 90, replace=False)\n",
    "test = np.random.choice([i for i in range(208) if i not in train_val], 58, replace=False) # 测试集索引\n",
    "# print(feature.shape)\n",
    "train_feature = feature[train]       \n",
    "train_label = label[train]       # 训练集\n",
    "test_feature = feature[test]\n",
    "test_label = label[test]         # 测试集\n",
    "val_feature = feature[val]        \n",
    "val_label = label[val]          # 验证集\n",
    "\n",
    "# 观察数据规模\n",
    "print(train_feature.shape)\n",
    "print(feature.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测数据正确率\n",
    "def right_rate(pred_label, label):\n",
    "    '''\n",
    "    input:\n",
    "    pred_label:预测的结果数组\n",
    "    label:实际的类别数组\n",
    "    output:\n",
    "    rate:正确率\n",
    "    '''\n",
    "    diff = pred_label - label\n",
    "    return diff.tolist().count(0) / len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        #决策树模型\n",
    "        self.tree = {}\n",
    "\n",
    "    # 计算信息熵\n",
    "    def cal_Ent(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        # 计算信息熵的pklogpk函数\n",
    "        def operate(pk):\n",
    "            '''\n",
    "            input:概率pk\n",
    "            output:pklogpk\n",
    "            '''\n",
    "            if pk == 0:           # 定义0log0=0\n",
    "                return 0\n",
    "            else:                 # 计算信息熵\n",
    "                return 0 - pk * math.log(pk, 2)\n",
    "        \n",
    "        varies = list(set(label))       # 含有的种类的列表（不重复）\n",
    "        p_labels = [label.tolist().count(vary) / len(label) for vary in varies]   # 每一个种类的概率矩阵\n",
    "\n",
    "        return sum(list(map(operate, p_labels)))      # 计算出信息熵\n",
    "    \n",
    "    # 计算信息增益\n",
    "    def cal_InfoGain(self, feature, label, index):\n",
    "        '''\n",
    "        input：\n",
    "         feature:特征数据集\n",
    "         label:标签集\n",
    "         index:feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "        output:信息增益\n",
    "        '''\n",
    "        \n",
    "        feature_ = np.array(feature).T[index]       # 把待查找的特征提取出来\n",
    "        Ent_old = self.cal_Ent(feature_, label)   # 这是总样本的信息熵\n",
    "\n",
    "        varies = list(set(feature_.tolist()))   # 讲样本该属性的种类列表找出来\n",
    "        feature_sorted = [np.where(feature_ == value)[0] for value in varies]  # 把分类后各小样本的索引列表在放到一个大列表中   \n",
    "\n",
    "        Ent_new = sum([self.cal_Ent(feature_[value], label[value]) * len(label[value]) / len(feature_) for value in feature_sorted])     # 这是分类后各小样本的信息熵之和               \n",
    "        \n",
    "        return Ent_old - Ent_new   # 返回信息增益\n",
    "\n",
    "    # 获得信息增益最高的特征\n",
    "    def getBestFeature(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        return np.argmax([self.cal_InfoGain(feature, label, index) for index in range(len(feature[0]))])   # 返回最大信息增益的特征的序列号\n",
    "    \n",
    "    # 创建决策树\n",
    "    def createTree(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        \n",
    "        if len(set(label)) == 1:           # 样本里都是同一个label没必要继续分叉了\n",
    "            return label[0]              # 直接作为叶子节点返回\n",
    "        \n",
    "        if len(feature[0]) == 1 or len(np.unique(feature, axis=0)) == 1:   # 样本中只有一个特征或者所有样本的特征都一样的话就看哪个label的数量比较多\n",
    "            return max(label, key=label.tolist().count)   # 返回出现次数最多的label\n",
    "        \n",
    "        best_feature = self.getBestFeature(feature, label)   # 根据信息增益得到特征的索引\n",
    "        tree = {best_feature: {}}      # 建立结点\n",
    "\n",
    "        feature_ = feature[:, best_feature]   # 将信息增益最大的特征的特征向量提取出来\n",
    "        varies = list(set(feature_))          # 含有所有的种类（不重复）的列表\n",
    "        sub_features = {vary: np.where(feature_ == vary)[0] for vary in varies}   # 把feature按照该特征分类，key是种类，value是索引的ndarray\n",
    "        for vary in varies:\n",
    "            tree[best_feature][vary] = self.createTree(feature[sub_features[vary]], label[sub_features[vary]])  # 递归求解构造新树 \n",
    "        \n",
    "        return tree\n",
    "\n",
    "    # 训练模型\n",
    "    def fit(self, feature, label):\n",
    "        '''\n",
    "        input: feature: 训练集数据\n",
    "         label:训练集标签\n",
    "        output: None\n",
    "        '''\n",
    "        self.tree = self.createTree(feature, label)\n",
    "\n",
    "    # 预测\n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        input: feature:测试集数据\n",
    "        output:预测结果，如np.array([0, 1, 2, 2, 1, 0])\n",
    "        '''\n",
    "        def judge_sing_feature(sing_feature, tree):\n",
    "            '''\n",
    "            input:sing_feature:单条数据\n",
    "            output:类别sing_label\n",
    "            '''\n",
    "            tree = tree\n",
    "            for k in tree.keys():\n",
    "                try:                      # 如果tree[k]这个字典中有sing_feature[k]这个关键字\n",
    "                    if isinstance(tree[k][sing_feature[k]], dict) == False:     # 不是字典类型就是值，则返回种类\n",
    "                        return tree[k][sing_feature[k]]          # 返回种类\n",
    "                    else:\n",
    "                        tree = judge_sing_feature(sing_feature, tree[k][sing_feature[k]])          # 是字典的话继续递归\n",
    "                except:                # 否则找出字典tree[k]的键中离sing_feature[k]最近的一个key\n",
    "                        k_ = list(tree[k].keys())[np.argmin(np.array([(key - sing_feature[k]) ** 2 for key in list(tree[k].keys())]))]   # 字典tree[k]的键中离sing_feature[k]最近的一个key\n",
    "                        if isinstance(tree[k][k_], dict) == False:      # 不是字典类型就是值，则返回种类\n",
    "                            return tree[k][k_]          # 返回种类\n",
    "                        else:\n",
    "                            tree = judge_sing_feature(sing_feature, tree[k][k_])  # 是字典的话继续递归\n",
    "            return tree\n",
    "        \n",
    "        return np.array([judge_sing_feature(sing_feature, self.tree) for sing_feature in feature])  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7586206896551724\n"
     ]
    }
   ],
   "source": [
    "A = DecisionTree()\n",
    "A.fit(train_feature, train_label)  # 训练决策树\n",
    "# print(A.tree[0])\n",
    "# print(A.tree)\n",
    "# for k in A.tree.keys():\n",
    "#     print(A.tree[k])\n",
    "\n",
    "pred_label = A.predict(test_feature)  # 预测\n",
    "# print(test_label)\n",
    "# print(pred_label)\n",
    "# print(test_label)\n",
    "# print(test_label)\n",
    "# print(train_label)\n",
    "# right_rate(pred_label, test_label)\n",
    "# print(A.tree)\n",
    "# print(pred_label.shape, label.shape)\n",
    "print(right_rate(pred_label, test_label))  # 输出正确率\n",
    "# right_rate(pred_label, test_label)\n",
    "# print(pred_label)\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪枝\n",
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        #决策树模型\n",
    "        self.tree = {}\n",
    "\n",
    "    # 计算信息熵\n",
    "    def cal_Ent(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        # 计算信息熵的pklogpk函数\n",
    "        def operate(pk):\n",
    "            '''\n",
    "            input:概率pk\n",
    "            output:pklogpk\n",
    "            '''\n",
    "            if pk == 0:           # 定义0log0=0\n",
    "                return 0\n",
    "            else:                 # 计算信息熵\n",
    "                return 0 - pk * math.log(pk, 2)\n",
    "        \n",
    "        if type(label) != type(np.array([])):\n",
    "            label = np.array(label)\n",
    "        if type(feature) != type(np.array([])):\n",
    "            feature = np.array(feature)                     # 类型统一转换\n",
    "        \n",
    "        varies = list(set(label))       # 含有的种类的列表（不重复）\n",
    "        p_labels = [label.tolist().count(vary) / len(label) for vary in varies]   # 每一个种类的概率矩阵\n",
    "\n",
    "        return sum(list(map(operate, p_labels)))      # 计算出信息熵\n",
    "    \n",
    "    # 计算信息增益\n",
    "    def cal_InfoGain(self, feature, label, index):\n",
    "        '''\n",
    "        input：\n",
    "         feature:特征数据集\n",
    "         label:标签集\n",
    "         index:feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。\n",
    "        output:信息增益\n",
    "        '''\n",
    "        if type(label) != type(np.array([])):\n",
    "            label = np.array(label)\n",
    "        if type(feature) != type(np.array([])):\n",
    "            feature = np.array(feature)                     # 类型统一转换\n",
    "        \n",
    "        feature_ = np.array(feature).T[index]       # 把待查找的特征提取出来\n",
    "        Ent_old = self.cal_Ent(feature_, label)   # 这是总样本的信息熵\n",
    "\n",
    "        varies = list(set(feature_.tolist()))   # 讲样本该属性的种类列表找出来\n",
    "        feature_sorted = [np.where(feature_ == value)[0] for value in varies]  # 把分类后各小样本的索引列表在放到一个大列表中   \n",
    "#         print(feature_sorted)\n",
    "#         print(type(feature_))\n",
    "#         print(feature_)\n",
    "#         print(type(label))\n",
    "        Ent_new = sum([self.cal_Ent(feature_[value], label[value]) * len(label[value]) / len(feature_) for value in feature_sorted])     # 这是分类后各小样本的信息熵之和               \n",
    "        \n",
    "        return Ent_old - Ent_new\n",
    "\n",
    "    # 获得信息增益最高的特征\n",
    "    def getBestFeature(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        return np.argmax([self.cal_InfoGain(feature, label, index) for index in range(len(feature[0]))])   # 返回最大信息增益的特征的序列号\n",
    "    \n",
    "    # 创建决策树\n",
    "    def createTree(self, feature, label):\n",
    "        '''\n",
    "        input:feature 特征数据集\n",
    "              label 标签集\n",
    "        output:信息熵\n",
    "        '''\n",
    "        if type(label) != type(np.array([])):\n",
    "            label = np.array(label)\n",
    "        if type(feature) != type(np.array([])):\n",
    "            feature = np.array(feature)                     # 类型统一转换\n",
    "        \n",
    "        if len(set(label)) == 1:           # 样本里都是同一个label没必要继续分叉了\n",
    "            return label[0]              # 直接作为叶子节点返回\n",
    "        \n",
    "        if len(feature) == 1 or len(np.unique(feature, axis=0)) == 1:   # 样本中只有一个特征或者所有样本的特征都一样的话就看哪个label的数量比较多\n",
    "            return max(label, key=label.tolist().count)   # 返回出现次数最多的label\n",
    "        \n",
    "        best_feature = self.getBestFeature(feature, label)   # 根据信息增益得到特征的索引\n",
    "        tree = {best_feature: {}}      # 建立结点\n",
    "\n",
    "        feature_ = feature[:, best_feature]   # 将信息增益最大的特征的特征向量提取出来\n",
    "        varies = list(set(feature_))          # 含有所有的种类（不重复）的列表\n",
    "        sub_features = {vary: np.where(feature_ == vary)[0] for vary in varies}   # 把feature按照该特征分类，key是种类，value是索引的ndarray\n",
    "        for vary in varies:\n",
    "            tree[best_feature][vary] = self.createTree(feature[sub_features[vary]], label[sub_features[vary]])  # 递归求解构造新树 \n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    # 计算验证集准确率\n",
    "    def calc_acc_val(self, tree, val_feature, val_label):\n",
    "        '''\n",
    "        input:\n",
    "        tree:决策树\n",
    "        val_feature:验证集的数据\n",
    "        val_label：验证集的标签集\n",
    "        output：决策树正确率\n",
    "        '''\n",
    "        def classify(tree, sing_feature):\n",
    "            '''\n",
    "            input: \n",
    "            tree:决策树\n",
    "            sing_feature:单条数据\n",
    "            output：正确率            \n",
    "            '''\n",
    "            if not isinstance(tree, dict):       # 如果tree直接就是值而不是字典，直接返回分类值\n",
    "                return tree\n",
    "            index, value = list(tree.items())[0]     # 得到tree的键和值\n",
    "            f_value = sing_feature[index]             # 得到该属性的feature的值，以便接下来归类\n",
    "            if isinstance(value, dict):       # 如果tree内含的值对应的是字典\n",
    "                try:                  # 如果tree[index]含有f_value的键\n",
    "                    classLabel = classify(tree[index][f_value], sing_feature)  # 递归查找\n",
    "                    return classLabel\n",
    "                except:              # 如果tree[index]不含有f_value的键，找到与其最接近的属性\n",
    "                    f_value_ = list(tree[index].keys())[np.argmin(np.array([(key - f_value) ** 2 for key in list(tree[index].keys())]))]   # 字典tree[index]的键中离sing_feature[index]最近的一个key\n",
    "                    classLabel = classify(tree[index][f_value_], feature)\n",
    "                    return classLabel\n",
    "            else:                         \n",
    "                return value           # 如果tree内含的值对应的不是字典，直接返回值\n",
    "            \n",
    "        return right_rate([classify(tree, sing_feature) for sing_feature in val_feature], val_label)   # 返回正确率\n",
    "    \n",
    "    \n",
    "\n",
    "    # 后剪枝\n",
    "    def post_cut(self, val_feature, val_label):\n",
    "        '''\n",
    "        input:\n",
    "        val_feature:验证集的数据\n",
    "        val_label：验证集的标签集\n",
    "        output：None\n",
    "        '''\n",
    "        \n",
    "        # 深度优先搜索\n",
    "        def dfs(tree, path, all_path):       # 深度优先搜索\n",
    "            '''\n",
    "            input: \n",
    "            tree:决策树\n",
    "            path:记录深度遍历的路径\n",
    "            all_path:记录每一条路径\n",
    "            output：None\n",
    "            '''\n",
    "            for k in tree.keys():          # 对于所有键（实际上是所有的子树）搜索\n",
    "                if isinstance(tree[k], dict):   # 如果是字典，也就是不是叶子节点\n",
    "                    path.append(k)         # 将该路径加进来\n",
    "                    dfs(tree[k], path, all_path)   # 递归调用\n",
    "                    if len(path) > 0:      # 如果path不为空\n",
    "                        path.pop()         # 如果遍历之后，那么就将其弹出，这样回退到最开始的时候path就为空，回退到上一级时候可以继续记录遍历同级的其他的子树了\n",
    "                else:\n",
    "                    all_path.append(path[:])   # 到最深处无法回溯后，将这一条路径放到all_path,得到了一个非叶子节点的路径\n",
    "        \n",
    "        # 拿到通往所有非叶子节点的路径\n",
    "        def get_non_leaf_node_count(tree):\n",
    "            '''\n",
    "            output:tree：生成的决策树\n",
    "            input: 决策树中通往所有非叶子节点的路径\n",
    "            '''\n",
    "            non_leaf_node_path = []\n",
    "            dfs(tree, [], non_leaf_node_path)    # 调用函数得到所有通往非叶子节点的路径。\n",
    "            unique_non_leaf_node = []     # 得到通往每一个叶子结点的路径，不重复\n",
    "            for path in non_leaf_node_path:\n",
    "                if path in unique_non_leaf_node:   # 如果已经有了\n",
    "                    continue                    # 没有任何操作，直接跳过\n",
    "                unique_non_leaf_node.append(path)   # 如果没有就加上\n",
    "            \n",
    "#             print(non_leaf_node_path)\n",
    "#             print(unique_non_leaf_node)\n",
    "            \n",
    "            return unique_non_leaf_node   # 返回所有路径\n",
    "        \n",
    "        \n",
    "        # 剪枝\n",
    "        def tree_cut(tree, path, max_label):\n",
    "            '''\n",
    "            input：\n",
    "            tree:当前决策树树\n",
    "            path:记录深度遍历的路径\n",
    "            max_label:原非叶子含有数目种类最多的标签\n",
    "            '''\n",
    "            for i in range(len(path)-1):      \n",
    "                tree = tree[path[i]]\n",
    "            tree[path[-1]] = max_label     # 非叶子节点赋值\n",
    "        \n",
    "#         path_visited = []    # 记录已经访问过的路径\n",
    "        all_path_ = get_non_leaf_node_count(self.tree)  # 记录通往所有非叶子结点的路径\n",
    "\n",
    "        acc_before_cut = self.calc_acc_val(self.tree, val_feature, val_label)   # 计算目前的正确率\n",
    "        # 遍历所有非叶子节点\n",
    "        for i in range(len(all_path_)):\n",
    "#             path = get_the_most_deep_path(self.tree)    # 得到最深的路径\n",
    "            path = all_path_[len(all_path_) - i - 1]\n",
    "#             path_visited.append(path)\n",
    "#             print(path)\n",
    "            \n",
    "            tree = deepcopy(self.tree)    # 将树完全复制一遍，另外开了一个存储空间，防止改变原树数据\n",
    "            step = deepcopy(tree)         # 同理\n",
    "            \n",
    "            for k in path:\n",
    "                step = step[k]      # 跟着路径走\n",
    "            \n",
    "            flag = False              # 判断是否该path是的子树全是叶子结点\n",
    "            for value in step.values():\n",
    "                if isinstance(value, dict):\n",
    "                    flag = True\n",
    "            if flag:         # 如果不是那么就返回\n",
    "                continue\n",
    "            \n",
    "            max_label = max(list(step.values()), key=list(step.values()).count)   # 叶子节点中票数最多的标签\n",
    "#             print(max_label)\n",
    "            \n",
    "            tree_cut(tree, path, max_label)           # 在备份的树上剪枝\n",
    "            acc_after_cut = self.calc_acc_val(tree, val_feature, val_label)   # 计算剪枝之后的正确率\n",
    "#             print(self.tree)\n",
    "#             print(tree)\n",
    "#             print('hello world')\n",
    "#             print(acc_after_cut, acc_before_cut)\n",
    "            \n",
    "            if acc_after_cut > acc_before_cut:            # 验证集准确率高于原来的就剪枝\n",
    "                tree_cut(self.tree, path, max_label)   # 剪枝\n",
    "                acc_before_cut = acc_after_cut            # 剪完后正确率更新\n",
    "#                 print('hello world')\n",
    "    \n",
    "    # 训练模型\n",
    "    def fit(self, train_feature, train_label, val_feature, val_label):\n",
    "        '''\n",
    "        train_feature: 训练集数据\n",
    "        train_label:训练集标签\n",
    "        val_feature:验证集的数据\n",
    "        val_label：验证集的标签集\n",
    "        output: None\n",
    "        '''\n",
    "        self.tree = self.createTree(train_feature, train_label)\n",
    "        self.post_cut(val_feature, val_label)   # 后剪枝\n",
    "        \n",
    "        \n",
    "    def predict(self, feature):\n",
    "        '''\n",
    "        input: feature:测试集数据，类型为ndarray\n",
    "        output:预测结果，如np.array([0, 1, 2, 2, 1, 0])\n",
    "        '''\n",
    "        result = []\n",
    "        # 单个样本分类\n",
    "        def classify(tree, feature):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "            t_index, t_value = list(tree.items())[0]\n",
    "            f_value = feature[t_index]\n",
    "            if isinstance(t_value, dict):\n",
    "                try:\n",
    "                    classLabel = classify(tree[t_index][f_value], feature)\n",
    "                    return classLabel\n",
    "                except:\n",
    "                    f_value_ = list(tree[t_index].keys())[np.argmin(np.array([(key - feature[t_index]) ** 2 for key in list(tree[t_index].keys())]))]\n",
    "                    classLabel = classify(tree[t_index][f_value_], feature)\n",
    "                    return classLabel\n",
    "            else:\n",
    "                return t_value\n",
    "        for f in feature:\n",
    "            result.append(classify(self.tree, f))\n",
    "        \n",
    "        # print(self.tree)\n",
    "        return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6896551724137931\n"
     ]
    }
   ],
   "source": [
    "A = DecisionTree()\n",
    "A.fit(train_feature=train_feature, train_label=train_label,val_feature=val_feature, val_label=val_label)\n",
    "# print(A.tree[0])\n",
    "# print(A.tree)\n",
    "# tree = A.tree\n",
    "# for k, v in tree:\n",
    "#     print(A.tree.keys())\n",
    "#     print(A.tree.values())\n",
    "#     tree = tree\n",
    "\n",
    "# for k in A.tree.keys():\n",
    "#     print(A.tree[k])\n",
    "\n",
    "pred_label = A.predict(test_feature)\n",
    "print(right_rate(pred_label, test_label))\n",
    "\n",
    "\n",
    "\n",
    "# a = [(min(sing), max(sing)) for sing in train_feature.T]\n",
    "# b = [(min(sing), max(sing)) for sing in test_feature.T]\n",
    "# print(np.array(a).shape)\n",
    "# for i, j in zip(a, b):\n",
    "#     print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
